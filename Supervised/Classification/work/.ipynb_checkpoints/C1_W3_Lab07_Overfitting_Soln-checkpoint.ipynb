{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ungraded Lab:  Overfitting in Logistic Regression.\n",
    "\n",
    "The lectures describe **Overfitting**. This is when the model follows the data too closely and does not generalize well. In this lab we will explore overfitting in logistic regression and how regularization can improve situation.\n"
   ],
   "id": "1f3527dc70cec0a9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline\n",
    "- [Tools](#tools)\n",
    "- [Dataset](#dataset)\n",
    "- [Polynomial Feature Map](#FeatureMap)\n",
    "- [Fit the Model](#FitModel)\n",
    "- [Reducing Overfitting](#ReduceOverfitting)"
   ],
   "id": "75690d99186a871d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting\n",
    "In this lab, we will explore how overfitting happens and what can be done about it.\n",
    "- Create a logistic dataset with an irregular boundary\n",
    "- Create an overfitting problem\n",
    "    - polynomial Regression and Feature mapping\n",
    "- Regularization to reduce overfitting\n",
    "<a name='tools'></a>\n",
    "## Tools \n",
    "- We have not yet developed all the capabilities to do gradient decent with regularization so we will utilized sklearn's LogisticRegression capabilities explored briefly in a previous lab. \n",
    "- Plotting is very useful when exploring decision boundaries. We will utilize matplotlib. Producing these plots is quite involved so helper routines are provided below.\n",
    "- We will create a polynomial feature set. `map_features` is provided to simplify that process"
   ],
   "id": "7723fb92eff60502"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression"
   ],
   "id": "2c2c66583771542f",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "source": [
    "def plot_data(X, y, pos_label=\"y=1\", neg_label=\"y=0\"):\n",
    "    positive = y == 1\n",
    "    negative = y == 0\n",
    "    \n",
    "    # Plot examples\n",
    "    plt.plot(X[positive, 0], X[positive, 1], 'k+', label=pos_label)\n",
    "    plt.plot(X[negative, 0], X[negative, 1], 'yo', label=neg_label)\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"X[:,0]\")\n",
    "    plt.ylabel(\"X[:,1]\")"
   ],
   "id": "e07e15ba16b350ca",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "def plot_decision_boundary(x0r,x1r, y,predict):\n",
    "    \"\"\"\n",
    "    Plots a decision boundary \n",
    "     Args:\n",
    "      x0r : (array_like Shape (1,1)) range (min, max) of x0\n",
    "      x1r : (array_like Shape (1,1)) range (min, max) of x1\n",
    "      y   : (array_like Shape (m, )) target values of y\n",
    "      predict : function to predict z values    \n",
    "    \"\"\"\n",
    "\n",
    "    h = .01  # step size in the mesh\n",
    "    # create a mesh to plot in\n",
    "    xx, yy = np.meshgrid(np.arange(x0r[0], x0r[1], h),\n",
    "                         np.arange(x0r[0], x0r[1], h))\n",
    "\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, m_max]x[y_min, y_max].\n",
    "    points = np.c_[xx.ravel(), yy.ravel()]\n",
    "    Xm = map_feature(points[:, 0], points[:, 1],degree)\n",
    "    Z = predict(Xm)\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contour(xx, yy, Z, colors='g') \n",
    "    #plot_data(X_orig,y_train)"
   ],
   "id": "ef1b07833e795814",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "def map_feature(X1, X2, degree):\n",
    "    \"\"\"\n",
    "    Feature mapping function to polynomial features    \n",
    "    \"\"\"\n",
    "    X1 = np.atleast_1d(X1)\n",
    "    X2 = np.atleast_1d(X2)\n",
    "\n",
    "    out = [np.ones(X1.shape[0])]\n",
    "    for i in range(1, degree+1):\n",
    "        for j in range(i + 1):\n",
    "            out.append((X1**(i-j) * (X2**j)))\n",
    "           \n",
    "    return np.stack(out, axis=1)\n"
   ],
   "id": "7498a688e12bee74",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='dataset'></a>\n",
    "##  Dataset\n",
    "Below we create a logistic dataset with two features based on a quadratic. Random noise is added to create a scenario where the model can overfit. "
   ],
   "id": "78370552bb7a653"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "source": [
    "m = 500\n",
    "n = 2\n",
    "np.random.seed(1)\n",
    "X_orig = 2*(np.random.rand(m,n)-[0.5,0.5])\n",
    "y_train =  X_orig[:,1]  > X_orig[:,0]**2 + 0.2*np.random.rand(m)   #quadratic + random\n",
    "plot_data(X_orig,y_train)\n",
    "plt.title(\"Logistic data set with quadratic boundary with noise\")\n",
    "plt.show()"
   ],
   "id": "5e99d6bded9af3fa",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='FeatureMap'></a>\n",
    "##  Create Overfitting...Polynomial Feature Mapping\n",
    "In real data sets, the boundary between \"True\" and \"False\" features is rarely a straight line. To create a non-linear decision boundary, our model will need to support non-linear features. Concretely, if we have two features in our feature set $x_1$ and $x_2$ we can build a model of degree 2:\n",
    "$$f_\\mathbf{w} = w_0 + w_1x_1 + w_2x_2 + w_3x_1^2 + w_4x_1x_2 + w_5x_2^2 \\tag{1} $$\n",
    "To do this, we must convert our two feature data set into a 6 feature data set, noting that,as usual, $x_0$ will be set to 1. The routine `map_feature` was provided above to do exactly this."
   ],
   "id": "934e224fc08a4c22"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "source": [
    "X_tmp = np.array([[2,0],[0,3],[2,3]] )  # values selected to illustrated equation\n",
    "print(\"Shape before feature mapping:\", X_tmp.shape)\n",
    "print(X_tmp, \"\\n\")\n",
    "\n",
    "mapped_X =  map_feature(X_tmp[:, 0], X_tmp[:, 1],degree = 2)\n",
    "\n",
    "print(\"Shape after feature mapping:\", mapped_X.shape)\n",
    "print(mapped_X)"
   ],
   "id": "5de66164efa6d0e0",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the results with equation (1) above.\n",
    "\n",
    "Of course, we don't have to stop at two. The `degree` argument to map_features will determine the degree of the polynomial that is created. The degree will be determined by the complexity of the curve you are trying to follow. Increasing the degree will allow the model to follow more irregular boundaries, but can also allow for overfitting. The number of features/parameters grows exponentially as all of the cross terms are included. Sklearn [`PolynomialFeatures`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) can also be used to create feature maps.\n",
    "\n",
    "Lets convert our dataset above to support degree 6."
   ],
   "id": "eab00a7d07daa096"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "source": [
    "print(\"Original shape of data:\", X_orig.shape)\n",
    "degree = 6\n",
    "mapped_X =  map_feature(X_orig[:, 0], X_orig[:, 1],degree)\n",
    "\n",
    "print(\"Shape after feature mapping:\", mapped_X.shape)"
   ],
   "id": "36604bc129708034",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, with a degree 6 polynomial, we now have 28 features!\n",
    "<a name='FitModel'></a>\n",
    "## Fit the model\n",
    "\n",
    "We are going to use the `LogisticRegression` feature of SkLearn that was introduced in a previous lab. One thing to note, this routine has regularization built in. We will enable and disable that capability to highlight aspects of over fitting. To disable it, the command line argument `penalty` is set to `none`. When enabled, the `C` command line argument controls how much regularization is used. "
   ],
   "id": "afe7cfdb8f5b68f7"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "source": [
    "# create and fit the model using our mapped_X feature set.\n",
    "lr = LogisticRegression(penalty='none', max_iter=10000)\n",
    "lr.fit(mapped_X,y_train)\n",
    "\n",
    "# print an evaluation of the fit, 1 is best.\n",
    "print(lr.score(mapped_X, y_train))"
   ],
   "id": "36eee26cc39784b9",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a trained model, lets map the Original Data (not predicted) along with the decision boundary we derive from the model. Examine `plot_decision_boundary` above to see the details of how this is accomplished."
   ],
   "id": "b84e99d01e12be1b"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "source": [
    "plot_decision_boundary([-1,1],[-1,1], y_train,lr.predict)\n",
    "plot_data(X_orig,y_train)\n",
    "plt.title(\"Example of overfitting, degree 6, no regularization\")\n",
    "plt.show()"
   ],
   "id": "8c5c2a0daa2f3961",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, the model has done an amazing job of separating the data! However, that is probably not what is desired. \n",
    "We can take two approaches to reducing overfitting:\n",
    "- regularization \n",
    "- reduce the degree of the polynomial."
   ],
   "id": "d206bba45165cf69"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ReduceOverfitting'></a>\n",
    "## Reducing Overfitting using regularization\n",
    "The next labs will cover regularization in more detail, so we will just explore this briefly.\n",
    "Lets fit the model again, but this time include regularization. "
   ],
   "id": "4f18afa8154697d2"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "source": [
    "# create and fit the model using our mapped_X feature set.\n",
    "lr = LogisticRegression(max_iter=1000, C=1)\n",
    "lr.fit(mapped_X,y_train)\n",
    "\n",
    "# print an evaluation of the fit, 1 is best.\n",
    "print(\"fitting score:\",lr.score(mapped_X, y_train))"
   ],
   "id": "c9d58a7aa570a6c8",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "source": [
    "plot_decision_boundary([-1,1],[-1,1], y_train,lr.predict)\n",
    "plot_data(X_orig,y_train)\n",
    "plt.title(\"Example of overfitting, degree 6, with regularization, C=1\")\n",
    "plt.show()"
   ],
   "id": "7991b81a4e3f9215",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision boundary is much more reasonable with some regularizationg.\n",
    "Change the value of `C` above to try more or less regularization. C must be strictly positive. Values less than 1 maximumize regularization while large values minimize regularization."
   ],
   "id": "7d4108128771e061"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce the degree of the polynomial\n",
    "A degree 6 polynomial may be more than is required! We can reduce the values to limit the model.\n",
    "To do this, we will need to regenerate our mapped data and refit the model."
   ],
   "id": "f2c2c62694b1360d"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "source": [
    "print(\"Original shape of data:\", X_orig.shape)\n",
    "degree = 2\n",
    "mapped_X =  map_feature(X_orig[:, 0], X_orig[:, 1],degree)\n",
    "\n",
    "print(\"Shape after feature mapping:\", mapped_X.shape)"
   ],
   "id": "4a9e557db7f5f06b",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "source": [
    "# create and fit the model using our mapped_X feature set.\n",
    "lr = LogisticRegression(penalty='none', max_iter=1000, C=1)\n",
    "lr.fit(mapped_X,y_train)\n",
    "\n",
    "# print an evaluation of the fit, 1 is best.\n",
    "print(\"fit score:\", lr.score(mapped_X, y_train))"
   ],
   "id": "3e0f16a03a889af1",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "source": [
    "plot_decision_boundary([-1,1],[-1,1], y_train,lr.predict)\n",
    "plot_data(X_orig,y_train)\n",
    "plt.title(\"Example of overfitting, degree 2, with no regularization\")\n",
    "plt.show()"
   ],
   "id": "75458cb753ebd9b3",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad! Of course, in this case, we knew ahead of time the data was quadratic and that a degree two polynomial would be a good choice. Try varying `degree` above to see the impact of polynomial degree on overfitting."
   ],
   "id": "73a7204a8bf21d81"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "id": "132e410e554bd6f5",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
