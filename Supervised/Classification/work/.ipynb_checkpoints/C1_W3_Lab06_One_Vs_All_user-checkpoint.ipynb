{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ungraded Lab:  Multiclass Classification: One-vs-all\n",
    "\n",
    "One vs All is one method for selection when there are more than two categories.\n",
    "![pic](./figures/onevsmany.png)"
   ],
   "id": "917f52b9d874892d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline\n",
    "- [Tools](#tools)\n",
    "- [Dataset](#dataset)\n",
    "- [One vs All Implementation](#ova)\n",
    "\n"
   ],
   "id": "4f716bcfe1591fe0"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass Classification: One-vs-all (OVA)\n",
    "In this lab, we will explore how to use the One-vs-All method for classification when there are more than two categories.This technique is an extention of two class or binomial logistic regression that we have working with. \n",
    "\n",
    "In binomail logistic regression, we train a model to classify samples that are in a class or not in a class. One-vs-All(OVA) extends this method by training $n$ models. Each model is responsible for identifying one class. A model for a given class is trained by recasting the training set to identify one class as positive and all the rest as negative. To make predictions, an example is processed by all $n$ models and the model with the largest prediction output is selected.\n",
    "\n",
    "In this lab, we will build a OVA classifier.\n",
    "## Tools \n",
    "- We will utilize our previous work to build and train models. These routines are provided. \n",
    "- Plotting decision boundaries and datasets is helpful. Producing these plots is quite involved so helper routines are provided below.\n",
    "- We will create a multi-class data set. Popular [`SkLearn`](https://scikit-learn.org/stable/) routines are utilized."
   ],
   "id": "18a51bf2c591232d"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "from lab_utils import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.linear_model import LogisticRegression"
   ],
   "id": "cb8b9b5db7368272",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "source": [
    "# Plot  multi-class training points\n",
    "def plot_mc_data(X, y, class_labels=None, legend=False):\n",
    "    classes = np.unique(y)\n",
    "    for i in classes:\n",
    "        label = class_labels[i] if class_labels else \"class {}\".format(i)\n",
    "        idx = np.where(y == i)\n",
    "        plt.scatter(X[idx, 0], X[idx, 1],  cmap=plt.cm.Paired,\n",
    "                    edgecolor='black', s=20, label=label)\n",
    "    if legend: plt.legend()"
   ],
   "id": "ff94fbde6e9b3b72",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These routines are provided but reviewing their operation is instructive. Plotting routines often make use of many esoteric but useful numpy routines. Plotting decision boundaries makes use of `matplotlib's` contour plot. A contour plot draws a line at boundary of a change in values. That capability is used to delineate changes in decisions. Briefly, the routine has 3 steps:\n",
    "- create a fine mesh of locations in a 2-D grid. Build an array of those points.\n",
    "- make predictions for each of those points. In this case, this includes the vote for the best prediction.\n",
    "- plot the mesh vs the predictions(`Z`) using a contour plot."
   ],
   "id": "7ed4fdfb927775ae"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "#Plot a multi-class decision boundary\n",
    "def plot_mc_decision_boundary(X,nclasses, Models , class_labels=None, legend=False):\n",
    "\n",
    "    # create a mesh to points to plot\n",
    "    h = 0.1  # step size in the mesh\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    points = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "    #make predictions for each point in mesh\n",
    "    z = np.zeros((len(points),nclasses))\n",
    "    Z = predict_mc(points,Models)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    #contour plot highlights boundaries between values - classes in this case\n",
    "    plt.figure()\n",
    "    plt.contour(xx, yy, Z, colors='g') \n",
    "    plt.axis('tight')\n"
   ],
   "id": "8b926190079618be",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're providing the routines which you have developed in previous labs to create and fit/train a model. Feel free to replace these with your own versions. (Keep a copy of the original just in case.)"
   ],
   "id": "fc01f113662b0c5b"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "def compute_gradient(X, y, w):\n",
    "    \n",
    "    m, n = X.shape\n",
    "    f = sigmoid(np.matmul(X, w))\n",
    "    dw = (1/m)*np.matmul(X.T, (f - y))\n",
    "\n",
    "    return dw"
   ],
   "id": "dc27eb56dc8c42b5",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "source": [
    "def compute_cost(X, y, w):\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    f = sigmoid(X @ w)\n",
    "    total_cost = (1/m)*(np.dot(-y, np.log(f)) - np.dot((1-y), np.log(1-f)))\n",
    "    \n",
    "    return total_cost"
   ],
   "id": "b430ba832d87b9f",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "source": [
    "def gradient_descent(X, y, w, cost_function, gradient_function, alpha, num_iters): \n",
    "    \"\"\"\n",
    "    Performs gradient descent to learn w. Updates w by taking \n",
    "    num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array_like\n",
    "        Shape (m, n+1) \n",
    "    \n",
    "    y : array_like\n",
    "        Shape (m,) \n",
    "    \n",
    "    w : array_like\n",
    "        Initial values of parameters of the model\n",
    "        Shape (n+1,)\n",
    "        \n",
    "    cost_function : function\n",
    "        Function that is used as cost function for optimization\n",
    "        Takes in parameters X, y, w\n",
    "        \n",
    "    gradient_function : function\n",
    "        Function that returns the gradient update at each step for \n",
    "        parameters w\n",
    "        Takes in parameters X, y, w\n",
    "        \n",
    "    alpha : float\n",
    "        Learning rate\n",
    "           \n",
    "    num_iters : int\n",
    "        number of iterations to run gradient descent\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    w : array_like\n",
    "        Shape (n+1,)\n",
    "        Updated values of parameters of the model after\n",
    "        running gradient descent\n",
    "        \n",
    "    J_history : array_like\n",
    "        Output of cost function at each iteration\n",
    "    \"\"\"\n",
    "    \n",
    "    # number of training examples\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # An array to store cost J at each iteration\n",
    "    J_history = np.zeros((num_iters, 1))\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        \n",
    "        # Save cost J at each iteration\n",
    "        J_history[i] = cost_function(X, y, w)\n",
    "        \n",
    "        # Calculate the gradient and update the parameters\n",
    "        gradient = gradient_function(X, y, w)\n",
    "        w = w - alpha * gradient\n",
    "          \n",
    "        # Print cost every 1000 iterations\n",
    "        if i%1000 == 0:\n",
    "            print(\"Cost at iteration %d: %f\" % (i, J_history[i]))\n",
    "        \n",
    "    return w, J_history"
   ],
   "id": "a1e788c0d87e9272",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "source": [
    "def predict(X, w): \n",
    "    \"\"\"\n",
    "    Predict whether the label is 0 or 1 using learned logistic\n",
    "    regression parameters w\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array_like\n",
    "        Shape (m, n+1) \n",
    "    \n",
    "    w : array_like\n",
    "        Parameters of the model\n",
    "        Shape (n+1, 1)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    p: array_like\n",
    "        Shape (m,)\n",
    "        The predictions for X using a threshold at 0.5\n",
    "    \"\"\"\n",
    "    # number of training examples\n",
    "    m = X.shape[0]   \n",
    "    p = np.zeros(m)\n",
    "   \n",
    "    for i in range(m):\n",
    "        f_w = sigmoid(np.dot(w.T, X[i]))\n",
    "        p[i] = f_w >=0.5\n",
    "    \n",
    "    return p"
   ],
   "id": "5c4dab01cfa17856",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='dataset'></a>\n",
    "##  Dataset\n",
    "Below, we use an `SkLearn` tool to create 3 'blobs' of data. Using NumPy's [`np.unique`](https://numpy.org/doc/stable/reference/generated/numpy.unique.html), we can look at the number and values of the classes."
   ],
   "id": "a5359b1c8993d50"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "source": [
    "# make 3-class dataset for classification\n",
    "centers = [[-5, 0], [0, 4.5], [5, -1]]\n",
    "X_orig, y_train = make_blobs(n_samples=500, centers=centers, cluster_std=0.85,random_state=40)\n"
   ],
   "id": "39865ee06b8b0550",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "source": [
    "plot_mc_data(X_orig,y_train,[\"blob one\", \"blob two\", \"blob three\"], legend=True)\n",
    "plt.show()"
   ],
   "id": "1b74f3181b47f318",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "source": [
    "# show classes in data set\n",
    "print(f\"unique classes {np.unique(y_train)}\")\n",
    "# show shapes of our dataset\n",
    "print(f\"shape of X_orig: {X_orig.shape}, shape of y_train: {y_train.shape}\")"
   ],
   "id": "ee32ec5140aacfc7",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ova'></a>\n",
    "##  One Vs All Implementation\n",
    "\n",
    "We'll implement the OVA algorithm in three step.\n",
    "- create and train three 'models'. Each trained to select one of the three classes.\n",
    "- create a routine that will use the models to make predictions and select the best answer.\n",
    "- plot the decision boundary using the prediction routine.\n",
    "\n",
    "### Step 1: Create and Train 3 models.\n",
    "The steps involved will be familiar from past labs utilizing gradient descent. For each class:\n",
    "- extend the data set with a column of ones to account for $w_0$ (this is provided)\n",
    "- create `w_init`, initial values from the parameters. We have 3 parameters.\n",
    "- call gradient descent. alpha=1e-2 and num_iters=1000 works well. This returns $w$ and Cost history. We won't need cost history here. $w$ is our model which we will store in an array.\n",
    "- call predict with the training data and our model ($w$) to see how good our model is. Note, predict expects the original, non-extended examples (`X_orig`).\n",
    "\n",
    "Below there is a for loop over each of the classes. \n",
    "- creates an target array with the current class set to one and all others set to zero.\n",
    "     - `yc = (y_train==classes[i]) + 0`\n",
    "- plots this interpretation of the data\n",
    "- your code\n",
    "- plots the predicted values\n",
    "Replace `None` with your code."
   ],
   "id": "79bf823f1b5af5f8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary><font size=\"2\" color=\"darkgreen\"><b>Hints</b></font></summary>\n",
    "\n",
    "```\n",
    "classes=[0,1,2]\n",
    "m,n = X_orig.shape\n",
    "# extend the data with a column of ones\n",
    "X_train = np.hstack([np.ones((m,1)), X_orig])\n",
    "# storage for our models (w), one column per class\n",
    "W_models = np.zeros((n+1,len(classes)))   \n",
    "\n",
    "plt.figure(figsize=(14, 14))             \n",
    "for i in classes:\n",
    "    ax = plt.subplot(3,2, 2*i + 1)\n",
    "    yc = (y_train==classes[i]) + 0\n",
    "    plot_mc_data(X_orig, yc,legend=True); plt.title(f\"Training Classes, class {i}\"); \n",
    "    ### START CODE HERE ### \n",
    "    w_init = np.zeros((3,))   \n",
    "    W_models[:,i],_ = gradient_descent(X_train, yc, w_init, compute_cost, compute_gradient,\n",
    "                                       alpha = 1e-2, num_iters=1000) \n",
    "    pred =  predict(X_train, W_models[:,i]) \n",
    "    ### END CODE HERE ###         \n",
    "    \n",
    "    ax = plt.subplot(3,2, 2*i + 2)\n",
    "    plot_mc_data(X_orig,pred,legend=True); plt.title(\"Predicted Classes\");\n",
    "plt.show   \n",
    "```\n",
    "</details>"
   ],
   "id": "8e369a5591d0fe21"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "source": [
    "classes=[0,1,2]\n",
    "m,n = X_orig.shape\n",
    "# extend the data with a column of ones\n",
    "X_train = np.hstack([np.ones((m,1)), X_orig])\n",
    "# storage for our models (w), one column per class\n",
    "W_models = np.zeros((n+1,len(classes)))   \n",
    "\n",
    "plt.figure(figsize=(14, 14))             \n",
    "for i in classes:\n",
    "    ax = plt.subplot(3,2, 2*i + 1)\n",
    "    yc = (y_train==classes[i]) + 0\n",
    "    plot_mc_data(X_orig, yc,legend=True); plt.title(f\"Training Classes, class {i}\"); \n",
    "    ### START CODE HERE ### \n",
    "\n",
    "    w_init = None  \n",
    "    W_models[:,i],_ = None  \n",
    "    pred = None  \n",
    "    ### END CODE HERE ###         \n",
    "    \n",
    "    ax = plt.subplot(3,2, 2*i + 2)\n",
    "    plot_mc_data(X_orig,pred,legend=True); plt.title(\"Predicted Classes\");\n",
    "plt.show"
   ],
   "id": "53474906251e95b9",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>\n",
    "    <b>**Expected Output**:</b>\n",
    "</summary>\n",
    "\n",
    " ![asdf](./figures/C1W3_trainvpredict.PNG)"
   ],
   "id": "3ec8404cda700a3e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have trained our 3 models we will write a routine to select the best prediction. \n",
    "-Step 1: Given $X$ and matrix $W$, perform a prediction resulting in three predictions. This can be one line if vectorised. ![pic](./figures/C1W3_XW.PNG)  \n",
    "-Step 2: use `np.argmax()` to return the **class** of the prediction with the highest value. Note that class is one of [0,1,2] and the index returned by `np.argmax` is, conveniently also [0,1,2]."
   ],
   "id": "7aa5683c4c638c8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary><font size=\"2\" color=\"darkgreen\"><b>Hints</b></font></summary>\n",
    "\n",
    "```\n",
    "def predict_mc(X,W):\n",
    "    \"\"\"\n",
    "    Adds a column of ones to X and computes n predictions and selects the best.\n",
    "    Args:\n",
    "      X : (array_like Shape (m,n)) feature values used in prediction.  \n",
    "      W : (array_like Shape (n,c)) Matrix of parameter. Each column represents 1 model.c models\n",
    "    Returns\n",
    "      sclass: (array_like Shape (m,1)) The selected class the values belong in. Values 0 to c.\n",
    "    \"\"\"\n",
    "    X = np.hstack([np.ones((len(X),1)), X])\n",
    "    ### START CODE HERE ### \n",
    "    P = X @ W\n",
    "    sclass = P.argmax(axis=1)\n",
    "    ### END CODE HERE ### \n",
    "\n",
    "    return(sclass)  \n",
    "```\n",
    "</details>"
   ],
   "id": "8014fc4348322b64"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "source": [
    "def predict_mc(X,W):\n",
    "    \"\"\"\n",
    "    Adds a column of ones to X and computes n predictions and selects the best.\n",
    "    Args:\n",
    "      X : (array_like Shape (m,n)) feature values used in prediction.  \n",
    "      W : (array_like Shape (n,c)) Matrix of parameter. Each column represents 1 model.c models\n",
    "    Returns\n",
    "      sclass: (array_like Shape (m,1)) The selected class the values belong in. Values 0 to c.\n",
    "    \"\"\"\n",
    "    X = np.hstack([np.ones((len(X),1)), X])\n",
    "    ### START CODE HERE ### \n",
    "\n",
    "    ### END CODE HERE ### \n",
    "\n",
    "    return(sclass)"
   ],
   "id": "e1db3c5db7c0f7ab",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we can make a prediction for any point, we can now produce a plot with the decision boundary shown."
   ],
   "id": "94244fa8ce33b172"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "source": [
    "#plot the decison boundary. Pass in our models - the w's assocated with each model\n",
    "plot_mc_decision_boundary(X_orig,3, W_models)\n",
    "plt.title(\"model decision boundary vs original training data\")\n",
    "\n",
    "#add the original data to the decison bounaryd\n",
    "plot_mc_data(X_orig,y_train,[\"blob one\", \"blob two\", \"blob three\"], legend=True)\n",
    "plt.show()"
   ],
   "id": "93bb44ac3d77ad79",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>\n",
    "    <b>**Expected Output**:</b>\n",
    "</summary>\n",
    "\n",
    "![sdf](./figures/C1W3_boundary.PNG)"
   ],
   "id": "27565a0b58990112"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There you are! You have now build a Multi-Class classifier.\n",
    "\n",
    "Lets try another case. We'll just move the blobs around a bit:\n",
    "## Second Test Case"
   ],
   "id": "f27693bfd1d99900"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "source": [
    "# make 3-class dataset for classification\n",
    "centers = [[-5, 0], [0, 1], [5, -1]]\n",
    "X_orig, y_train = make_blobs(n_samples=500, centers=centers, cluster_std=1.2,random_state=40)\n"
   ],
   "id": "75b6d0fc4738f3ad",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "source": [
    "plot_mc_data(X_orig,y_train,[\"blob one\", \"blob two\", \"blob three\"], legend=True)\n",
    "plt.show()"
   ],
   "id": "3f02e2d24a692598",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "source": [
    "# show classes in data set\n",
    "print(f\"unique classes {np.unique(y_train)}\")\n",
    "# show shapes of our dataset\n",
    "print(f\"shape of X_orig: {X_orig.shape}, shape of y_train: {y_train.shape}\")"
   ],
   "id": "4d7c5fc78aece143",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examaning the plot above, do you see any potential issues with our current approach?\n",
    "\n",
    "Piece together the pieces from above, or create subroutines to create a decision boundary diagram like the one in the first example."
   ],
   "id": "61e7869294797476"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary><font size=\"2\" color=\"darkgreen\"><b>Hints</b></font></summary>\n",
    "\n",
    "```\n",
    "classes=[0,1,2]\n",
    "m,n = X_orig.shape\n",
    "X_train = np.hstack([np.ones((m,1)), X_orig])\n",
    "W_models = np.zeros((n+1,len(classes)))   # stores our models\n",
    "             \n",
    "for i in classes:\n",
    "    y_ = (y_train==classes[i]) + 0\n",
    "    #plot_mc_data(X_orig, y_,legend=True); plt.title(\"Original Classes\"); plt.show()\n",
    "    w_init = np.zeros((3,))\n",
    "    W_models[:,i],_ = gradient_descent(X_train, y_, w_init, compute_cost, compute_gradient, 1e-3, 10) \n",
    "    pred =  predict(X_train, W_models[:,i])\n",
    "    #plot_mc_data(X_orig,pred,legend=True); plt.title(\"Predicted Classes\"); plt.show()\n",
    "    \n",
    "#plot the decison boundary. Pass in our models - the w's assocated with each model\n",
    "plot_mc_decision_boundary(X_orig,3, W_models)\n",
    "plt.title(\"model decision boundary vs original training data\")\n",
    "\n",
    "#add the original data to the decison boundary\n",
    "plot_mc_data(X_orig,y_train,[\"blob one\", \"blob two\", \"blob three\"], legend=True)\n",
    "plt.show() \n",
    "```\n",
    "</details>"
   ],
   "id": "963cc44d5726776f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "#Rewrite code here"
   ],
   "id": "6b1a117cea89966c",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>\n",
    "    <b>**Expected Output**:</b>\n",
    "</summary>\n",
    "\n",
    "![asdf](./figures/C1W3_example2.PNG)\n",
    "    \n",
    "We will study logistic regression with polynomial features in the next lab. That will allow us to handle situations where purely linear solutions are not enough."
   ],
   "id": "e3afacc03f2b300d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook made was informed by an example at scikit-learn.org. The author was Tom Dupre la Tour <tom.dupre-la-tour@m4x.org>"
   ],
   "id": "3acdab839245f936"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "id": "612fee4894a8016e",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
